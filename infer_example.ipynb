{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision.transforms import ToPILImage\n",
    "import random\n",
    "from functools import partial, update_wrapper\n",
    "from types import MethodType\n",
    "from typing import Any\n",
    "import os\n",
    "from torch import Tensor\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.pre_processing.transforms import Denormalize\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from anomalib.data import InferenceDataset\n",
    "from anomalib.data.folder import Folder\n",
    "from anomalib.models.fastflow.lightning_model import Fastflow\n",
    "from anomalib.post_processing import superimpose_anomaly_map\n",
    "from anomalib.pre_processing.transforms import Denormalize\n",
    "from anomalib.utils.callbacks.visualizer import BaseVisualizerCallback\n",
    "from anomalib.utils.callbacks import (\n",
    "    LoadModelCallback,\n",
    "    MetricsConfigurationCallback,\n",
    "    MinMaxNormalizationCallback,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"fastflow\"  \n",
    "CONFIG_PATH = <PATH TO THE CONFIG FILE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_configurable_parameters(config_path=CONFIG_PATH)\n",
    "IMAGE_SIZE = config[\"dataset\"][\"image_size\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)\n",
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(**config.trainer, callbacks=callbacks);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to denormalize the image for visualization\n",
    "\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "def denormalize2(tensor):\n",
    "    meantens = Tensor(IMG_MEAN)\n",
    "    stdtens = Tensor(IMG_STD)\n",
    "    if tensor.dim() == 4:\n",
    "        if tensor.size(0):\n",
    "            tensor = tensor.squeeze(0)\n",
    "  \n",
    "    mean = IMG_MEAN\n",
    "    std = IMG_STD\n",
    "    newtensor = torch.empty(3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "    newtensor[0] = tensor[0] * std[0] + mean[0]\n",
    "    newtensor[1] = tensor[1] * std[1] + mean[1]\n",
    "    newtensor[2] = tensor[2] * std[2] + mean[2]\n",
    "    \n",
    "    \n",
    "    array = (newtensor * 255).permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the path to the folder containing the images you want to infer on and the path to the checkpoint you want to use\n",
    "\n",
    "NORMAL_IMAGES_PATH = <PATH TO NORMAL IMAGES>\n",
    "ABNORMAL_IMAGES_PATH = <PATH TO ABNORMAL IMAGES>\n",
    "CHECKPOINT_PATH = <PATH TO MODEL CHECKPOINT FILE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction function of trainer returns a dictionary with following keys:\n",
    "\n",
    "- *image* contains a tensor of the image itself\n",
    "- *image_path* contains the image's path\n",
    "- *anomaly_maps* contains a tensor of the anomaly map of the image\n",
    "- *pred_scores* contains a tensor of the anomaly score\n",
    "- *pred_labels* contains a tensor of a bool meaning the predicted class\n",
    "- *pred_mask* contains a tensor of booleans values. It is equal to the anomaly map where the pixels > 0.5 are set to true and the rest to false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all the images of NORMAL_IMAGES_PATH and ABNORMAL_IMAGES_PATH and store the results in a DataFrame.\n",
    "\n",
    "df = DataFrame(columns=['prediction', 'target', 'image_path'])\n",
    "\n",
    "for images_path in [NORMAL_IMAGES_PATH,ABNORMAL_IMAGES_PATH]:\n",
    "    if images_path == ABNORMAL_IMAGES_PATH:\n",
    "        print(\"starting to predict abnormal images\")\n",
    "    print(images_path)\n",
    "    normal_abnormal_images_path = os.listdir(images_path)\n",
    "    print(len(normal_abnormal_images_path))\n",
    "\n",
    "    inference_dataloader = InferenceDataset(\n",
    "        path=images_path,\n",
    "        image_size=IMAGE_SIZE)\n",
    "    inference_dataloader = DataLoader(dataset=inference_dataloader)\n",
    "\n",
    "    predictions = trainer.predict(model=model,ckpt_path=CHECKPOINT_PATH, dataloaders=inference_dataloader)\n",
    "\n",
    "\n",
    "    if images_path == NORMAL_IMAGES_PATH:\n",
    "        target = \"Normal\"\n",
    "    else:\n",
    "        target = \"Abnormal\"\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        df = df.append({'prediction':predictions[i]['pred_scores'].item(), 'target': target,'image_path': predictions[i]['image_path']}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphic to visualise the distribution of the predictions\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.violinplot(y=df['prediction'], x=df['target'], inner=\"quartile\", split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_IMAGES_TO_SHOW = 5\n",
    "r = list(range(len(df)))\n",
    "random.shuffle(r)\n",
    "\n",
    "for i in r[:NUMBER_OF_IMAGES_TO_SHOW]:\n",
    "        PATHOFIMAGETOANALYSE = df[\"image_path\"].tolist()[i][0]\n",
    "        TARGET = df[\"target\"].tolist()[i]\n",
    "        PREDICTION_SCORE = df[\"prediction\"].tolist()[i]\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(16 ,4), linewidth=0)\n",
    "\n",
    "\n",
    "        #############\n",
    "        # FILE PATH #\n",
    "        #############\n",
    "        inference_dataset = InferenceDataset(\n",
    "        path=PATHOFIMAGETOANALYSE,\n",
    "        image_size=IMAGE_SIZE)\n",
    "        inference_dataloader = DataLoader(dataset=inference_dataset);\n",
    "\n",
    "        predictions = trainer.predict(model=model,ckpt_path=CHECKPOINT_PATH, dataloaders=inference_dataloader)[0];\n",
    "\n",
    "        print(PATHOFIMAGETOANALYSE)\n",
    "        print(TARGET)\n",
    "        print(PREDICTION_SCORE)\n",
    "\n",
    "\n",
    "        #########\n",
    "        # IMAGE #\n",
    "        #########\n",
    "        image = predictions[\"image\"][0]\n",
    "        image = denormalize2(image)\n",
    "        ax0 = fig.add_subplot(1, 4, 1)\n",
    "        ax0.axis(\"off\")\n",
    "        ax0.imshow(image)\n",
    "        ax0.set_title(f'\\n\\n {TARGET} Original Image')\n",
    "\n",
    "\n",
    "        ###############\n",
    "        # ANOMALY MAP #\n",
    "        ###############\n",
    "        anomaly_map = predictions[\"anomaly_maps\"][0]\n",
    "        anomaly_map = anomaly_map.cpu().numpy().squeeze()\n",
    "        ax1 = fig.add_subplot(1, 4,2)\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(anomaly_map)\n",
    "        ax1.set_title(f'\\n\\nAnomaly map with AS = {PREDICTION_SCORE * 100:.1f}%')\n",
    "\n",
    "\n",
    "        ###########################\n",
    "        # SUPERIMPOSE ANOMALY MAP #\n",
    "        ###########################\n",
    "        heat_map = superimpose_anomaly_map(anomaly_map=anomaly_map, image=image, normalize=True)\n",
    "        ax2 = fig.add_subplot(1, 4, 3)\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(heat_map)\n",
    "        ax2.set_title('\\n\\nHeat map')\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # PREDICTION MASK #\n",
    "        ###################\n",
    "        pred_masks = predictions[\"pred_masks\"][0].squeeze().cpu().numpy()\n",
    "        ax3 = fig.add_subplot(1, 4, 4)\n",
    "        ax3.axis('off')\n",
    "        ax3.imshow(pred_masks)\n",
    "        ax3.set_title('\\n\\nMask')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomaly score is here defined as the number of pixels of the anomaly map higher than 0.5 divided by the image dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarea = DataFrame(columns=['prediction', 'target', 'image_path'])\n",
    "lstpred = []\n",
    "\n",
    "for images_path in [NORMAL_IMAGES_PATH,ABNORMAL_IMAGES_PATH]:\n",
    "    if images_path == ABNORMAL_IMAGES_PATH:\n",
    "        print(\"starting to predict abnormal images\")\n",
    "    print(images_path)\n",
    "    normal_abnormal_images_path = os.listdir(images_path)\n",
    "    print(len(normal_abnormal_images_path))\n",
    "\n",
    "    inference_dataloader = InferenceDataset(\n",
    "        path=images_path,\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "    inference_dataloader = DataLoader(dataset=inference_dataloader, num_workers=16)\n",
    "\n",
    "    predictions = trainer.predict(model=model,ckpt_path=CHECKPOINT_PATH, dataloaders=inference_dataloader)\n",
    "\n",
    "    lstpred = lstpred + predictions\n",
    "\n",
    "    if images_path == NORMAL_IMAGES_PATH:\n",
    "        target = \"Normal\"\n",
    "    else:\n",
    "        target = \"Abnormal\"\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        dfarea = dfarea.append({'prediction':predictions[i]['pred_scores'].item(), 'target': target,'image_path': predictions[i]['image_path']}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the anomaly score as defined above and rewrite the prediction score in the dataframe.\n",
    "\n",
    "anomaldf = dfarea.copy(deep=True)\n",
    "pixthresh = 0.5 # Set pixel threshold value equal to 0.5 which is equal to the threshold used for the mask.\n",
    "\n",
    "for s in range(len(lstpred)):\n",
    "    anomal_number = 0\n",
    "    anomaly_map = lstpred[s][\"anomaly_maps\"]\n",
    "    anomaly_map = anomaly_map.cpu().numpy().squeeze()\n",
    "    for i in range(IMAGE_SIZE):\n",
    "        for j in range(IMAGE_SIZE):   \n",
    "                if anomaly_map[i][j] > pixthresh:\n",
    "                    anomal_number += 1\n",
    "    anomal_score = anomal_number/(IMAGE_SIZE*IMAGE_SIZE)\n",
    "    anomaldf.loc[anomaldf.index == s, 'prediction'] = anomal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphic to visualise the distribution of the predictions\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.violinplot(y=anomaldf['prediction'], x=anomaldf['target'], inner=\"quartile\", split=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
